{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5649f41b-f94a-46dd-b14a-07091d6fd133",
   "metadata": {},
   "source": [
    "# Consigna: Práctica 7 Ejercicio 4\n",
    "\n",
    "### Implementación y evaluación con GRU\n",
    "\n",
    "Modifique la implementación previa para **usar GRU (Gated Recurrent Units)**.\n",
    "Luego, ejecute una serie de **experimentos** para evaluar las diferencias en desempeño respecto de la implementación anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da32a9b-9f43-4248-b1e5-c3f9afb84004",
   "metadata": {},
   "source": [
    "# Resolucion\n",
    "\n",
    "En este ejercicio se implementó una red neuronal recurrente (RNN) desde cero, tomando como base el repositorio de Andrej Karpathy, con el objetivo de entender el funcionamiento interno de este tipo de redes.\n",
    "La RNN fue entrenada sobre un conjunto de texto para predecir el siguiente carácter en una secuencia, lo que permitió observar cómo el modelo aprende dependencias temporales.\n",
    "Luego, la implementación se extendió a una versión GRU (Gated Recurrent Unit), reemplazando las operaciones básicas de la RNN por las ecuaciones que incorporan puertas de actualización y reinicio, lo que mejora la capacidad de mantener información a largo plazo (ventaja principal de las GRU) y evita en parte el problema del gradiente desvanecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a71651-027f-4016-94a0-f101e4ec5b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 58 characters, 21 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Entrada/Salida de datos\n",
    "data = open('input.txt', 'r').read() # archivo de texto plano de entrada, MINIMO 1MB (1.000.000 caracteres aprox), para ver mejoria en GRU.\n",
    "chars = list(set(data)) # caracteres unicos en el archivo de entrada\n",
    "data_size, vocab_size = len(data), len(chars) # calcula la cantidad total de caracteres y el tamaño del vocabulario\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # diccionario: caracter -> indice\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # diccionario: indice -> caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f6f692-9c38-4b77-9603-918ff5dd3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros comunes a RNN y GRU\n",
    "hidden_size = 256 # tamaño de la capa oculta (cantidad de neuronas)\n",
    "seq_length = 15 # numero de pasos para desenrollar la RNN (longitud de secuencia)\n",
    "learning_rate = 0.001\n",
    "np.random.seed(42) # semilla fija para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180860b6-18f2-4145-9686-0b3f97339c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros del modelo RNN\n",
    "rnn_Wxh = np.random.randn(hidden_size, vocab_size)*0.1 # pesos: entrada -> capa oculta\n",
    "rnn_Whh = np.random.randn(hidden_size, hidden_size)*0.1 # pesos: capa oculta -> capa oculta (memoria)\n",
    "rnn_Why = np.random.randn(vocab_size, hidden_size)*0.1 # pesos: capa oculta -> salida\n",
    "rnn_bh = np.zeros((hidden_size, 1)) # sesgo de la capa oculta\n",
    "rnn_by = np.zeros((vocab_size, 1)) # sesgo de la capa de salida\n",
    "\n",
    "# Memoria Adagrad para RNN\n",
    "# Se usan para acumular los cuadrados de los gradientes y modificar la tasa de aprendizaje\n",
    "rnn_mWxh = np.zeros_like(rnn_Wxh)\n",
    "rnn_mWhh = np.zeros_like(rnn_Whh)\n",
    "rnn_mWhy = np.zeros_like(rnn_Why)\n",
    "rnn_mbh = np.zeros_like(rnn_bh)\n",
    "rnn_mby = np.zeros_like(rnn_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52366434-4aad-451e-b441-1b7880f1a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para el modelo RNN\n",
    "def rnn_lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    Calcula la pérdida y los gradientes de una RNN simple.\n",
    "\n",
    "    Parámetros:\n",
    "        inputs (list[int]): índices de los caracteres de entrada.\n",
    "        targets (list[int]): índices esperados como salida.\n",
    "        hprev (ndarray): estado oculto previo (hidden_size, 1).\n",
    "\n",
    "    Retorna:\n",
    "        loss (float): pérdida total.\n",
    "        dWxh, dWhh, dWhy, dbh, dby (ndarray): gradientes de los parámetros.\n",
    "        hprev (ndarray): último estado oculto.\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # Forward pass\n",
    "    # Recorre la secuencia de entrada, actualizando el estado oculto y calculando las probabilidades de salida.\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1)) # vector one-hot para el caracter de entrada\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh( # estado oculto actual\n",
    "            np.dot(rnn_Wxh, xs[t]) + # influencia de la entrada actual\n",
    "            np.dot(rnn_Whh, hs[t-1]) + # influencia del estado anterior\n",
    "            rnn_bh # sesgo/bias\n",
    "        )\n",
    "        ys[t] = np.dot(rnn_Why, hs[t]) + rnn_by # salida sin normalizar\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilidades (softmax)\n",
    "        loss += -np.log(ps[t][targets[t], 0]) # perdida de entropia cruzada acumulada\n",
    "    \n",
    "    # Backward pass\n",
    "    # Calcula los gradientes de todos los parametros mediante retropropagacion en el tiempo (BPTT)\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(rnn_Wxh), np.zeros_like(rnn_Whh), np.zeros_like(rnn_Why)\n",
    "    dbh, dby = np.zeros_like(rnn_bh), np.zeros_like(rnn_by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # gradiente de la perdida\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(rnn_Why.T, dy) + dhnext # gradiente del estado oculto\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # derivada de tanh\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(rnn_Whh.T, dhraw) # propaga el gradiente hacia atras en el tiempo\n",
    "\n",
    "    # Limita los gradientes para evitar explosion de gradientes\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def rnn_sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    Genera una secuencia de caracteres con la RNN.\n",
    "\n",
    "    Parámetros:\n",
    "        h (ndarray): estado oculto inicial.\n",
    "        seed_ix (int): índice del caracter inicial.\n",
    "        n (int): cantidad de caracteres a generar.\n",
    "\n",
    "    Retorna:\n",
    "        list[int]: índices de los caracteres generados.\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    # Genera n caracteres de salida uno por uno\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(rnn_Wxh, x) + np.dot(rnn_Whh, h) + rnn_bh) # actualiza el estado oculto\n",
    "        y = np.dot(rnn_Why, h) + rnn_by # calcula la salida\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) # convierte a probabilidad (softmax)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) # muestra el siguiente caracter segun p\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1 # actualiza la entrada con el nuevo caracter\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6013e2cf-7db6-4ad0-876f-19281f776e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros del modelo GRU\n",
    "\n",
    "# Compuerta de actualizacion (update gate)\n",
    "# Controla cuanto del estado anterior se conserva\n",
    "gru_Wxz = np.random.randn(hidden_size, vocab_size)*0.1 # pesos: entrada -> z\n",
    "gru_Whz = np.random.randn(hidden_size, hidden_size)*0.1 # pesos: estado oculto -> z\n",
    "gru_bz = np.zeros((hidden_size, 1)) # sesgo de la compuerta z\n",
    "\n",
    "# Compuerta de reset (reset gate)\n",
    "# Decide cuanto del estado anterior se ignora al calcular el nuevo candidato\n",
    "gru_Wxr = np.random.randn(hidden_size, vocab_size)*0.1 # pesos: entrada -> r\n",
    "gru_Whr = np.random.randn(hidden_size, hidden_size)*0.1 # pesos: estado oculto -> r\n",
    "gru_br = np.zeros((hidden_size, 1)) # seesgo de la compuerta r\n",
    "\n",
    "# Candidato de estado oculto\n",
    "# Representa la posible nueva informacion a incorporar en el estado\n",
    "gru_Wxh = np.random.randn(hidden_size, vocab_size)*0.1 # pesos: entrada -> ĥ\n",
    "gru_Whh = np.random.randn(hidden_size, hidden_size)*0.1 # pesos: estado oculto -> ĥ\n",
    "gru_bh = np.zeros((hidden_size, 1)) # sesgo del candidato ĥ\n",
    "\n",
    "# Capa de salida\n",
    "gru_Why = np.random.randn(vocab_size, hidden_size)*0.1 # pesos: estado oculto -> salida\n",
    "gru_by = np.zeros((vocab_size, 1)) # sesgo de la salida\n",
    "\n",
    "# Memoria Adagrad\n",
    "# Acumulan los cuadrados de los gradientes para ajustar la tasa de aprendizaje\n",
    "gru_mWxz = np.zeros_like(gru_Wxz)\n",
    "gru_mWhz = np.zeros_like(gru_Whz)\n",
    "gru_mbz = np.zeros_like(gru_bz)\n",
    "gru_mWxr = np.zeros_like(gru_Wxr)\n",
    "gru_mWhr = np.zeros_like(gru_Whr)\n",
    "gru_mbr = np.zeros_like(gru_br)\n",
    "gru_mWxh = np.zeros_like(gru_Wxh)\n",
    "gru_mWhh = np.zeros_like(gru_Whh)\n",
    "gru_mbh = np.zeros_like(gru_bh)\n",
    "gru_mWhy = np.zeros_like(gru_Why)\n",
    "gru_mby = np.zeros_like(gru_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917916e9-417e-4665-b9d9-f89c64dd4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para el modelo GRU\n",
    "def gru_sigmoid(x):\n",
    "    \"\"\"Función sigmoide para las compuertas\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def gru_lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    Calcula pérdida y gradientes para una GRU (forward + backward).\n",
    "\n",
    "    Parámetros:\n",
    "        inputs (list[int]): índices de entrada.\n",
    "        targets (list[int]): índices objetivo.\n",
    "        hprev (ndarray): estado oculto previo (hidden_size, 1).\n",
    "\n",
    "    Devuelve:\n",
    "        loss (float): pérdida total\n",
    "        dWxz, dWhz, dbz,\n",
    "        dWxr, dWhr, dbr,\n",
    "        dWxh, dWhh, dbh,\n",
    "        dWhy, dby (ndarray): gradientes en ese orden,\n",
    "        hprev (ndarray): último estado oculto.\n",
    "    \"\"\"\n",
    "    xs, hs, hs_tilde, zs, rs, ys, ps = {}, {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # Forward pass\n",
    "    # Recorre la secuencia, calcula compuertas, candidato, nuevo estado y probabilidades\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1)) # vector one-hot de entrada\n",
    "        xs[t][inputs[t]] = 1\n",
    "        \n",
    "        # Compuerta de actualización z\n",
    "        zs[t] = gru_sigmoid(np.dot(gru_Wxz, xs[t]) + np.dot(gru_Whz, hs[t-1]) + gru_bz)\n",
    "        \n",
    "        # Compuerta de reset r\n",
    "        rs[t] = gru_sigmoid(np.dot(gru_Wxr, xs[t]) + np.dot(gru_Whr, hs[t-1]) + gru_br)\n",
    "        \n",
    "        # Candidato de estado oculto ĥ\n",
    "        hs_tilde[t] = np.tanh(\n",
    "            np.dot(gru_Wxh, xs[t]) +\n",
    "            np.dot(gru_Whh, rs[t] * hs[t-1]) +\n",
    "            gru_bh\n",
    "        )\n",
    "        \n",
    "        # Nuevo estado oculto (interpolacion entre estado anterior y candidato)\n",
    "        hs[t] = (1 - zs[t]) * hs[t-1] + zs[t] * hs_tilde[t]\n",
    "        \n",
    "        # Calcular salida\n",
    "        ys[t] = np.dot(gru_Why, hs[t]) + gru_by # puntiaciones sin normalizar\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilidades (softmax)\n",
    "        \n",
    "        # Calcular pérdida\n",
    "        loss += -np.log(ps[t][targets[t], 0])\n",
    "    \n",
    "    # Backward pass (BPTT)\n",
    "    dWxz = np.zeros_like(gru_Wxz)\n",
    "    dWhz = np.zeros_like(gru_Whz)\n",
    "    dbz = np.zeros_like(gru_bz)\n",
    "    dWxr = np.zeros_like(gru_Wxr)\n",
    "    dWhr = np.zeros_like(gru_Whr)\n",
    "    dbr = np.zeros_like(gru_br)\n",
    "    dWxh = np.zeros_like(gru_Wxh)\n",
    "    dWhh = np.zeros_like(gru_Whh)\n",
    "    dbh = np.zeros_like(gru_bh)\n",
    "    dWhy = np.zeros_like(gru_Why)\n",
    "    dby = np.zeros_like(gru_by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Gradiente de la salida\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        dh = np.dot(gru_Why.T, dy) + dhnext # gradiente respecto al estado oculto actual\n",
    "        \n",
    "        # Gradiente del candidato ĥ\n",
    "        dh_tilde = dh * zs[t] # la parte que paso por la compuerta z\n",
    "        dh_tilde_raw = (1 - hs_tilde[t] * hs_tilde[t]) * dh_tilde # derivada de la tanh\n",
    "        dbh += dh_tilde_raw\n",
    "        dWxh += np.dot(dh_tilde_raw, xs[t].T)\n",
    "        dWhh += np.dot(dh_tilde_raw, (rs[t] * hs[t-1]).T)\n",
    "        \n",
    "        # Gradiente de la compuerta de reset r\n",
    "        dr = np.dot(gru_Whh.T, dh_tilde_raw) * hs[t-1]\n",
    "        dr_raw = dr * rs[t] * (1 - rs[t]) # derivada sigmoide\n",
    "        dbr += dr_raw\n",
    "        dWxr += np.dot(dr_raw, xs[t].T)\n",
    "        dWhr += np.dot(dr_raw, hs[t-1].T)\n",
    "        \n",
    "        # Gradiente de la compuerta de actualización z\n",
    "        dz = dh * (hs_tilde[t] - hs[t-1])\n",
    "        dz_raw = dz * zs[t] * (1 - zs[t]) # derivada sigmoide\n",
    "        dbz += dz_raw\n",
    "        dWxz += np.dot(dz_raw, xs[t].T)\n",
    "        dWhz += np.dot(dz_raw, hs[t-1].T)\n",
    "        \n",
    "        # Gradiente hacia el estado oculto anterior\n",
    "        dhnext = dh * (1 - zs[t]) # parte que viene por la interpolacion\n",
    "        dhnext += np.dot(gru_Whz.T, dz_raw) # contribucion via z\n",
    "        dhnext += np.dot(gru_Whr.T, dr_raw) # contribucion via r\n",
    "        dhnext += np.dot(gru_Whh.T, dh_tilde_raw) * rs[t] # contribucion via el candidato\n",
    "    \n",
    "    # Recortar gradientes para prevenir explosion\n",
    "    for dparam in [dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby, hs[len(inputs)-1]\n",
    "\n",
    "def gru_sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    Genera una secuencia de índices usando la GRU.\n",
    "\n",
    "    Parámetros:\n",
    "        h (ndarray): estado oculto inicial.\n",
    "        seed_ix (int): índice inicial.\n",
    "        n (int): longitud de secuencia a generar.\n",
    "\n",
    "    Retorna:\n",
    "        list[int]: índices generados.\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    # Genera n indices uno por uno\n",
    "    for t in range(n):\n",
    "        z = gru_sigmoid(np.dot(gru_Wxz, x) + np.dot(gru_Whz, h) + gru_bz)\n",
    "        r = gru_sigmoid(np.dot(gru_Wxr, x) + np.dot(gru_Whr, h) + gru_br)\n",
    "        h_tilde = np.tanh(np.dot(gru_Wxh, x) + np.dot(gru_Whh, r * h) + gru_bh)\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        \n",
    "        y = np.dot(gru_Why, h) + gru_by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b77e89-a045-4d7c-8043-bbe2db45587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ReRteldsRolsptn \n",
      "----\n",
      "iter 0, loss: 45.668363\n",
      "----\n",
      " a tclenpara ant \n",
      "----\n",
      "iter 100, loss: 43.260331\n",
      "----\n",
      " ender como fufc \n",
      "----\n",
      "iter 200, loss: 39.886482\n",
      "----\n",
      " tstNis idrutb   \n",
      "----\n",
      "iter 300, loss: 36.577647\n",
      "----\n",
      " simp e para ent \n",
      "----\n",
      "iter 400, loss: 33.468061\n",
      "----\n",
      " onlor como funs \n",
      "----\n",
      "iter 500, loss: 30.593511\n",
      "----\n",
      " nmno de prueeae \n",
      "----\n",
      "iter 600, loss: 27.958030\n",
      "----\n",
      " simple pars ent \n",
      "----\n",
      "iter 700, loss: 25.540579\n",
      "----\n",
      " endeo como func \n",
      "----\n",
      "iter 800, loss: 23.331845\n",
      "----\n",
      " lsto ue pouebn  \n",
      "----\n",
      "iter 900, loss: 21.320464\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 1000, loss: 19.482369\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 1100, loss: 17.807353\n",
      "----\n",
      " tpTt de pruebl  \n",
      "----\n",
      "iter 1200, loss: 16.285298\n",
      "----\n",
      " simplefpara ent \n",
      "----\n",
      "iter 1300, loss: 14.895703\n",
      "----\n",
      " enler como func \n",
      "----\n",
      "iter 1400, loss: 13.630595\n",
      "----\n",
      " xrto dn pruebn  \n",
      "----\n",
      "iter 1500, loss: 12.482352\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 1600, loss: 11.434071\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 1700, loss: 10.480020\n",
      "----\n",
      " axto de prueba  \n",
      "----\n",
      "iter 1800, loss: 9.614802\n",
      "----\n",
      " simpleepara ent \n",
      "----\n",
      "iter 1900, loss: 8.824474\n",
      "----\n",
      " enser como func \n",
      "----\n",
      "iter 2000, loss: 8.105158\n",
      "----\n",
      " Nxto dd prueba  \n",
      "----\n",
      "iter 2100, loss: 7.453237\n",
      "----\n",
      " simple parabent \n",
      "----\n",
      "iter 2200, loss: 6.857116\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 2300, loss: 6.314366\n",
      "----\n",
      " axto de prueba  \n",
      "----\n",
      "iter 2400, loss: 5.822740\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 2500, loss: 5.372492\n",
      "----\n",
      " ender como fuuc \n",
      "----\n",
      "iter 2600, loss: 4.962292\n",
      "----\n",
      " rato de prueba  \n",
      "----\n",
      "iter 2700, loss: 4.590926\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 2800, loss: 4.250096\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 2900, loss: 3.939295\n",
      "----\n",
      " itto dr prufbad \n",
      "----\n",
      "iter 3000, loss: 3.658067\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 3100, loss: 3.399261\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 3200, loss: 3.162965\n",
      "----\n",
      " Txtoodd pruebb  \n",
      "----\n",
      "iter 3300, loss: 2.949274\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 3400, loss: 2.751950\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 3500, loss: 2.571507\n",
      "----\n",
      " xxto de prueba  \n",
      "----\n",
      "iter 3600, loss: 2.408428\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 3700, loss: 2.257209\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 3800, loss: 2.118660\n",
      "----\n",
      " oxto dl prueba  \n",
      "----\n",
      "iter 3900, loss: 1.993532\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 4000, loss: 1.876919\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 4100, loss: 1.769827\n",
      "----\n",
      " x to deoprueba  \n",
      "----\n",
      "iter 4200, loss: 1.673189\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 4300, loss: 1.582588\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 4400, loss: 1.499156\n",
      "----\n",
      " lxto de prueba  \n",
      "----\n",
      "iter 4500, loss: 1.423938\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 4600, loss: 1.352929\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 4700, loss: 1.287332\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 4800, loss: 1.228256\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 4900, loss: 1.172043\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 5000, loss: 1.119929\n",
      "----\n",
      " pxto de prueba  \n",
      "----\n",
      "iter 5100, loss: 1.073053\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 5200, loss: 1.028054\n",
      "----\n",
      " enderlcomo func \n",
      "----\n",
      "iter 5300, loss: 0.986172\n",
      "----\n",
      " mnto de Trueba  \n",
      "----\n",
      "iter 5400, loss: 0.948551\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 5500, loss: 0.912088\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 5600, loss: 0.878008\n",
      "----\n",
      " Txto de prueba  \n",
      "----\n",
      "iter 5700, loss: 0.847441\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 5800, loss: 0.817511\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 5900, loss: 0.789415\n",
      "----\n",
      " sxto dNmprueba  \n",
      "----\n",
      "iter 6000, loss: 0.764256\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 6100, loss: 0.739360\n",
      "----\n",
      " ender comp func \n",
      "----\n",
      "iter 6200, loss: 0.715887\n",
      "----\n",
      " uxto de prueba  \n",
      "----\n",
      "iter 6300, loss: 0.694903\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 6400, loss: 0.673917\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 6500, loss: 0.654044\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 6600, loss: 0.636311\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 6700, loss: 0.618389\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 6800, loss: 0.601348\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 6900, loss: 0.586172\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7000, loss: 0.570678\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 7100, loss: 0.555889\n",
      "----\n",
      " Rxtx de prusba  \n",
      "----\n",
      "iter 7200, loss: 0.542744\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7300, loss: 0.529197\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 7400, loss: 0.516220\n",
      "----\n",
      " pxto de prueba  \n",
      "----\n",
      "iter 7500, loss: 0.504709\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7600, loss: 0.492743\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 7700, loss: 0.481244\n",
      "----\n",
      " lxdo de pruebat \n",
      "----\n",
      "iter 7800, loss: 0.471065\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7900, loss: 0.460399\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8000, loss: 0.450121\n",
      "----\n",
      " pxto de prueba  \n",
      "----\n",
      "iter 8100, loss: 0.441043\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 8200, loss: 0.431461\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8300, loss: 0.422206\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 8400, loss: 0.414050\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 8500, loss: 0.405383\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8600, loss: 0.396997\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 8700, loss: 0.389622\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 8800, loss: 0.381740\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8900, loss: 0.374100\n",
      "----\n",
      " exoo de prueba  \n",
      "----\n",
      "iter 9000, loss: 0.367396\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 9100, loss: 0.360192\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 9200, loss: 0.353201\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 9300, loss: 0.347080\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 9400, loss: 0.340471\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 9500, loss: 0.334049\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 9600, loss: 0.328441\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 9700, loss: 0.322357\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 9800, loss: 0.316439\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 9900, loss: 0.311284\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento RNN\n",
    "rnn_n = 0 # contador de iteraciones\n",
    "rnn_p = 0 # puntero a la posicion de los datos\n",
    "rnn_smooth_loss = -np.log(1.0/vocab_size)*seq_length # perdida inicial estimada\n",
    "rnn_num_iterations = 10000 # numero total de iteraciones de entrenamiento\n",
    "\n",
    "while rnn_n < rnn_num_iterations:\n",
    "    # Reiniciar estado al llegar al final del texto o en la primera iteracion\n",
    "    if rnn_p + seq_length + 1 >= len(data) or rnn_n == 0:\n",
    "        rnn_hprev = np.zeros((hidden_size, 1)) # estado oculto inicial cero\n",
    "        rnn_p = 0\n",
    "\n",
    "    # Preparar la secuencia de entrada y sus targets (desplazada 1 paso)\n",
    "    inputs = [char_to_ix[ch] for ch in data[rnn_p:rnn_p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[rnn_p+1:rnn_p+seq_length+1]]\n",
    "    \n",
    "    # Generar y mostrar una muestra de texto cada 100 iteraciones\n",
    "    if rnn_n % 100 == 0:\n",
    "        sample_ix = rnn_sample(rnn_hprev, inputs[0], 15)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "    \n",
    "    # Calcular pérdida y gradientes por BPTT\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, rnn_hprev = rnn_lossFun(inputs, targets, rnn_hprev)\n",
    "    rnn_smooth_loss = rnn_smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    if rnn_n % 100 == 0:\n",
    "        print('iter %d, loss: %f' % (rnn_n, rnn_smooth_loss))\n",
    "    \n",
    "    # Actualizar parámetros con Adagrad\n",
    "    for param, dparam, mem in zip(\n",
    "        [rnn_Wxh, rnn_Whh, rnn_Why, rnn_bh, rnn_by],\n",
    "        [dWxh, dWhh, dWhy, dbh, dby],\n",
    "        [rnn_mWxh, rnn_mWhh, rnn_mWhy, rnn_mbh, rnn_mby]\n",
    "    ):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    # Avanzar el puntero y el contador\n",
    "    rnn_p += seq_length\n",
    "    rnn_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de926a4-5e91-40db-ab81-b4d442adb220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " rctnldpretNflrN \n",
      "----\n",
      "iter 0, loss: 45.667708\n",
      "----\n",
      "  lpu pe la  ena \n",
      "----\n",
      "iter 100, loss: 44.677952\n",
      "----\n",
      " tneo  nmaox mnc \n",
      "----\n",
      "iter 200, loss: 42.881238\n",
      "----\n",
      " csetfideeruaaa  \n",
      "----\n",
      "iter 300, loss: 40.707875\n",
      "----\n",
      "  emmpp raae ntt \n",
      "----\n",
      "iter 400, loss: 38.314101\n",
      "----\n",
      " seder mooomunnc \n",
      "----\n",
      "iter 500, loss: 35.831218\n",
      "----\n",
      " pdxa  edeureaaa \n",
      "----\n",
      "iter 600, loss: 33.361867\n",
      "----\n",
      " simole para ent \n",
      "----\n",
      "iter 700, loss: 30.944579\n",
      "----\n",
      " tnder comp fpnc \n",
      "----\n",
      "iter 800, loss: 28.632656\n",
      "----\n",
      " xieble drrrbebm \n",
      "----\n",
      "iter 900, loss: 26.462537\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 1000, loss: 24.423851\n",
      "----\n",
      " ender somo func \n",
      "----\n",
      "iter 1100, loss: 22.529194\n",
      "----\n",
      " areue  e pruuea \n",
      "----\n",
      "iter 1200, loss: 20.784839\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 1300, loss: 19.166095\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 1400, loss: 17.674406\n",
      "----\n",
      " oaottde prueua  \n",
      "----\n",
      "iter 1500, loss: 16.309552\n",
      "----\n",
      " simple para edt \n",
      "----\n",
      "iter 1600, loss: 15.047449\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 1700, loss: 13.887774\n",
      "----\n",
      " dx impee rreba  \n",
      "----\n",
      "iter 1800, loss: 12.829575\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 1900, loss: 11.851878\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 2000, loss: 10.954516\n",
      "----\n",
      " sl oote erueba  \n",
      "----\n",
      "iter 2100, loss: 10.136971\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 2200, loss: 9.381373\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 2300, loss: 8.688078\n",
      "----\n",
      " xpo de prueba s \n",
      "----\n",
      "iter 2400, loss: 8.057189\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 2500, loss: 7.473490\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 2600, loss: 6.937855\n",
      "----\n",
      " mxad de prueba  \n",
      "----\n",
      "iter 2700, loss: 6.450950\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 2800, loss: 5.999727\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 2900, loss: 5.585483\n",
      "----\n",
      " omdo de prueba  \n",
      "----\n",
      "iter 3000, loss: 5.209334\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 3100, loss: 4.859974\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 3200, loss: 4.539021\n",
      "----\n",
      " moxoo  epprueba \n",
      "----\n",
      "iter 3300, loss: 4.247929\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 3400, loss: 3.976792\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 3500, loss: 3.727458\n",
      "----\n",
      " xpN ee pruera a \n",
      "----\n",
      "iter 3600, loss: 3.501623\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 3700, loss: 3.290513\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 3800, loss: 3.096133\n",
      "----\n",
      " oltptde prueba  \n",
      "----\n",
      "iter 3900, loss: 2.920345\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 4000, loss: 2.755294\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 4100, loss: 2.603083\n",
      "----\n",
      " ostmode prueba  \n",
      "----\n",
      "iter 4200, loss: 2.465681\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 4300, loss: 2.335987\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 4400, loss: 2.216154\n",
      "----\n",
      " xxmo de prueba  \n",
      "----\n",
      "iter 4500, loss: 2.108212\n",
      "----\n",
      " silple para ent \n",
      "----\n",
      "iter 4600, loss: 2.005683\n",
      "----\n",
      " eeder como func \n",
      "----\n",
      "iter 4700, loss: 1.910739\n",
      "----\n",
      " ntoodd  ruebaa  \n",
      "----\n",
      "iter 4800, loss: 1.825431\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 4900, loss: 1.743805\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 5000, loss: 1.668021\n",
      "----\n",
      " mfoo  e prueba  \n",
      "----\n",
      "iter 5100, loss: 1.600130\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 5200, loss: 1.534617\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 5300, loss: 1.473617\n",
      "----\n",
      " eito de prueba  \n",
      "----\n",
      "iter 5400, loss: 1.419153\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 5500, loss: 1.366093\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 5600, loss: 1.316526\n",
      "----\n",
      " opoa ed rrueea  \n",
      "----\n",
      "iter 5700, loss: 1.272441\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 5800, loss: 1.229032\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 5900, loss: 1.188338\n",
      "----\n",
      " itot de prueba  \n",
      "----\n",
      "iter 6000, loss: 1.152298\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 6100, loss: 1.116397\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 6200, loss: 1.082612\n",
      "----\n",
      " utlp de prueba  \n",
      "----\n",
      "iter 6300, loss: 1.052832\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 6400, loss: 1.022794\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 6500, loss: 0.994415\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 6600, loss: 0.969525\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 6700, loss: 0.944091\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 6800, loss: 0.919963\n",
      "----\n",
      " xxto de prueba  \n",
      "----\n",
      "iter 6900, loss: 0.898913\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7000, loss: 0.877113\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 7100, loss: 0.856349\n",
      "----\n",
      " aTxoo de prueba \n",
      "----\n",
      "iter 7200, loss: 0.838331\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7300, loss: 0.819419\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 7400, loss: 0.801335\n",
      "----\n",
      " bxto de prueba  \n",
      "----\n",
      "iter 7500, loss: 0.785727\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7600, loss: 0.769128\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 7700, loss: 0.753196\n",
      "----\n",
      " bexto de prueba \n",
      "----\n",
      "iter 7800, loss: 0.739519\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 7900, loss: 0.724789\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8000, loss: 0.710600\n",
      "----\n",
      " ex o de prueba  \n",
      "----\n",
      "iter 8100, loss: 0.698481\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 8200, loss: 0.685275\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8300, loss: 0.672512\n",
      "----\n",
      " mRto de prueba  \n",
      "----\n",
      "iter 8400, loss: 0.661665\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 8500, loss: 0.649713\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8600, loss: 0.638130\n",
      "----\n",
      " lTotodde prueba \n",
      "----\n",
      "iter 8700, loss: 0.628329\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 8800, loss: 0.617422\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 8900, loss: 0.606825\n",
      "----\n",
      " Nmxto de prueba \n",
      "----\n",
      "iter 9000, loss: 0.597896\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 9100, loss: 0.587870\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 9200, loss: 0.578107\n",
      "----\n",
      " exto de prueba  \n",
      "----\n",
      "iter 9300, loss: 0.569912\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 9400, loss: 0.560638\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 9500, loss: 0.551589\n",
      "----\n",
      " xto de prueba   \n",
      "----\n",
      "iter 9600, loss: 0.544020\n",
      "----\n",
      " simple para ent \n",
      "----\n",
      "iter 9700, loss: 0.535394\n",
      "----\n",
      " ender como func \n",
      "----\n",
      "iter 9800, loss: 0.526964\n",
      "----\n",
      " tnato eeprreea  \n",
      "----\n",
      "iter 9900, loss: 0.519936\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento GRU\n",
    "gru_n = 0\n",
    "gru_p = 0\n",
    "gru_smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "gru_num_iterations = 10000\n",
    "\n",
    "while gru_n < gru_num_iterations:\n",
    "    # Reiniciar estado al llegar al final del texto o en la primera iteracion\n",
    "    if gru_p + seq_length + 1 >= len(data) or gru_n == 0:\n",
    "        gru_hprev = np.zeros((hidden_size, 1))\n",
    "        gru_p = 0\n",
    "\n",
    "    # Preparar la secuencia de entrada y sus targets (desplazada 1 paso)\n",
    "    inputs = [char_to_ix[ch] for ch in data[gru_p:gru_p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[gru_p+1:gru_p+seq_length+1]]\n",
    "    \n",
    "    # Generar y mostrar una muestra de texto cada 100 iteraciones\n",
    "    if gru_n % 100 == 0:\n",
    "        sample_ix = gru_sample(gru_hprev, inputs[0], 15)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "    \n",
    "    # Calcular pérdida y gradientes por BPTT\n",
    "    loss, dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby, gru_hprev = \\\n",
    "        gru_lossFun(inputs, targets, gru_hprev)\n",
    "    gru_smooth_loss = gru_smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    if gru_n % 100 == 0:\n",
    "        print('iter %d, loss: %f' % (gru_n, gru_smooth_loss))\n",
    "    \n",
    "    # Actualizar parámetros con Adagrad\n",
    "    params = [gru_Wxz, gru_Whz, gru_bz, gru_Wxr, gru_Whr, gru_br,\n",
    "             gru_Wxh, gru_Whh, gru_bh, gru_Why, gru_by]\n",
    "    dparams = [dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby]\n",
    "    mems = [gru_mWxz, gru_mWhz, gru_mbz, gru_mWxr, gru_mWhr, gru_mbr,\n",
    "           gru_mWxh, gru_mWhh, gru_mbh, gru_mWhy, gru_mby]\n",
    "    \n",
    "    for param, dparam, mem in zip(params, dparams, mems):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    # Avanzar el puntero y el contador\n",
    "    gru_p += seq_length\n",
    "    gru_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d82cdae-14f0-4db5-a06a-f527f0ef3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss final RNN: 0.3059\n",
      "Loss final GRU: 0.5123\n",
      "Muestra final RNN\n",
      "\n",
      "oxcn xo mo fm p\n",
      "\n",
      "\n",
      "Muestra final GRU\n",
      "\n",
      "axso de prueba \n"
     ]
    }
   ],
   "source": [
    "# Comparacion\n",
    "print(f\"Loss final RNN: {rnn_smooth_loss:.4f}\")\n",
    "print(f\"Loss final GRU: {gru_smooth_loss:.4f}\")\n",
    "\n",
    "# Muestra final generada por la RNN (estado inicial cero y semilla aleatoria)\n",
    "h = np.zeros((hidden_size, 1))\n",
    "sample_ix = rnn_sample(h, np.random.randint(vocab_size), 15)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print(\"Muestra final RNN\\n\")\n",
    "print(txt)\n",
    "print(\"\\n\");\n",
    "\n",
    "# Muestra final generada por la GRU (estado inicial cero y semilla aleatoria)\n",
    "h = np.zeros((hidden_size, 1))\n",
    "sample_ix = gru_sample(h, np.random.randint(vocab_size), 15)\n",
    "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "print(\"Muestra final GRU\\n\")\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba512e3-e5e4-494a-ab39-fe151078cf24",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "En este experimento usamos hiperparámetros comunes a la RNN y a la GRU con el objetivo de comparar cómo cada red, con los mismos parámetros, logra converger.\n",
    "\n",
    "La RNN simple alcanzó un loss menor (0.3059) comparado con la GRU (0.5123) bajo los mismos parámetros de entrenamiento. Esto ocurre porque **la RNN tiene muchos menos parámetros de pesos que la GRU**, por ende puede converger más rápidamente. \n",
    "Sin embargo, hay que tener en cuenta que **un loss menor no necesariamente significa un mejor desempeño en la tarea**.\n",
    "\n",
    "A pesar de tener un loss mayor, la GRU generó texto que claramente es mejor que el de la RNN:\n",
    "\n",
    "- **GRU**: `\"axso de prueba\"` → genera palabras con sentido en español como \"prueba\"\n",
    "- **RNN**: `\"oxcn xo mo fm p\"` → secuencia de caracteres sin sentido alguno\n",
    "\n",
    "De esto se puede entender que **la GRU, sin haber convergido del todo, ya capta mejor las dependencias a largo plazo** y la estructura del lenguaje.\n",
    "\n",
    "Los resultados evidencian el clásico **trade-off**:\n",
    "\n",
    "- **RNN**: optimiza más rápido con menos iteraciones (útil cuando hay restricciones de tiempo)\n",
    "- **GRU**: necesita más tiempo para converger, pero aprende representaciones más complejas del lenguaje\n",
    "\n",
    "También se puede apreciar un poco de **underfitting** en la GRU. Entendemos que su estructura es bastante compleja y está diseñada para textos mucho más grandes de los que estamos usando en este experimento. \n",
    "Con más datos y más iteraciones de entrenamiento, la GRU probablemente superaría a la RNN tanto en loss como en calidad de texto generado.\n",
    "\n",
    "# Reflexión final\n",
    "\n",
    "Este experimento demuestra que **elegir el modelo correcto depende del contexto**: si tenemos recursos limitados, la RNN puede ser suficiente. Pero si queremos modelar lenguaje de forma más realista y tenemos recursos suficientes, vale la pena invertir en entrenar una GRU por más tiempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
