{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85a952-cebe-4876-9d8e-80b581615ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 58 characters, 21 unique.\n",
      "----\n",
      " RrRibsnNlbtfsorNixRibRdniNcRRRamRTstsTcdiuaduNucxceudmoxttpTpdeRtTpfnmofll uRitTpnmruaNeTxpbcfRreixsopixsRnmsctffndno nmtbmdieob iTNNciRanceuRNb RicaanfardcmRiuslcnissNasrxRnTenrbroabdmbRcbNrN lbNrm   \n",
      "----\n",
      "iter 0, loss: 76.113061\n",
      "----\n",
      " exto te a onirtlcfpa eenndonneenna xntplce nitenaoe raeantenda Ra pabauntie i prnamnifenc enata s ona rapd ucdpsce nctenctRarfendereioronee cip a andfpncionce ncfonttentee a elba on enaepaTfondpunnron \n",
      "----\n",
      "iter 100, loss: 74.770845\n",
      "----\n",
      " exta de pruera ettenbmpa elsimple pa funciona plba ertenaruedr simploere fundmmndrp e par  entendeleco simpae para simple para entensee cRma funcioneae prueba fcttnaioeb papae cora funcionbauebde como \n",
      "----\n",
      "iter 200, loss: 69.183198\n",
      "----\n",
      " exto de prueba simple para enteed ple paio simple parapo amo funciona para entender co siisecf scmple para entender como funciona pnbar  funciona parar cornnfunciona plndsara simple para entencer como \n",
      "----\n",
      "iter 300, loss: 62.890405\n",
      "----\n",
      " exto de prueba simple plra entender como funaiona para entender como funciona para entender como t nciona para entender como funciona para entender como funciona para eba simple para entender como fun \n",
      "----\n",
      "iter 400, loss: 57.027020\n",
      "----\n",
      " exto de prueba simple para entendem como fusRiona para entender como funamona para entender como funciona para entender como funciona e como funciona para entenderacomo funciona para entender comoafun \n",
      "----\n",
      "iter 500, loss: 51.672752\n",
      "----\n",
      " exto de prueba simple para entender como funciona para entender como funciona para entender como funciona para entender como funciona para entenprueba siipne plra entender como funciona para entender  \n",
      "----\n",
      "iter 600, loss: 46.805604\n",
      "----\n",
      " exto de prueba simple para entender como funciona para entender e para entender como funciona para entender como funciena para entender como tunciona para entender como funciona para entender como fun \n",
      "----\n",
      "iter 700, loss: 42.389385\n",
      "----\n",
      " exto de prueba simple para entender coms funciona e a pa a entender como funciona para entender como funciona para entender como funciona para entender como funciona para entender como funciona par  e \n",
      "----\n",
      "iter 800, loss: 38.385486\n",
      "----\n",
      " exto de prueba simple para entender como funcionadrara entender como funciona para cntundiona para entender como funciona para endend pan paco simple para entender como funciona para entender como fun \n",
      "----\n",
      "iter 900, loss: 34.757039\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Entrada/Salida de datos\n",
    "data = open('input.txt', 'r').read() # archivo de texto plano de entrada\n",
    "chars = list(set(data)) # caracteres unicos en el archivo de entrada\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # diccionario: caracter -> indice\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # diccionario: indice -> caracter\n",
    "\n",
    "# Hiperparametros\n",
    "hidden_size = 100 # tamaño de la capa oculta (cantidad de neuronas)\n",
    "seq_length = 25 # numero de pasos para desenrollar la RNN (longitud de secuencia)\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# parametros del modelo (matrices de pesos y sesgos)\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # pesos: entrada -> capa oculta\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # pesos: capa oculta -> capa oculta (memoria)\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # pesos: capa oculta -> salida\n",
    "bh = np.zeros((hidden_size, 1)) # sesgo de la capa oculta\n",
    "by = np.zeros((vocab_size, 1)) # sesgo de la capa de salida\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  Calcula la pérdida (loss) y los gradientes para actualizar los pesos.\n",
    "  \n",
    "  Parámetros:\n",
    "  - inputs: lista de índices de caracteres de entrada\n",
    "  - targets: lista de índices de caracteres objetivo (siguiente carácter)\n",
    "  - hprev: estado oculto anterior (memoria de la red)\n",
    "  \n",
    "  Retorna:\n",
    "  - loss: valor de la función de pérdida\n",
    "  - gradientes para cada parámetro (dWxh, dWhh, dWhy, dbh, dby)\n",
    "  - último estado oculto\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {} # diccionarios para almacenar valores en cada paso temporal\n",
    "  hs[-1] = np.copy(hprev) # inicializar con el estado oculto anterior\n",
    "  loss = 0\n",
    "  \n",
    "  # Propagacion hacia adelante (forward pass)\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # codificar entrada en representacion one-hot\n",
    "    xs[t][inputs[t]] = 1 # poner un 1 en la posicion del caracter actual\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # calcular estado oculto\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # calcular salida (logits sin normalizar)\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # aplicar softmax: convertir a probabilidades\n",
    "    loss += -np.log(ps[t][targets[t],0]) # calcular perdida con entropia cruzada\n",
    "  \n",
    "  # Propagacion hacia atras (backward pass): calcular gradientes en orden inverso\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0]) # gradiente del siguiente estado oculto\n",
    "  \n",
    "  for t in reversed(range(len(inputs))): # recorrer desde el final hasta el inicio\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # gradiente de la funcion softmax con entropia cruzada\n",
    "    dWhy += np.dot(dy, hs[t].T) # gradiente de Why\n",
    "    dby += dy # gradiente de by\n",
    "    dh = np.dot(Why.T, dy) + dhnext # gradiente que llega a la capa oculta\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # gradiente a traves de la funcion tanh\n",
    "    dbh += dhraw # gradiente de bh\n",
    "    dWxh += np.dot(dhraw, xs[t].T) # gradiente de Wxh\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) # gradiente de Whh\n",
    "    dhnext = np.dot(Whh.T, dhraw) # gradiente para el paso temporal anterior\n",
    "  \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # recortar gradientes para evitar explosion\n",
    "  \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  Genera una secuencia de caracteres del modelo entrenado.\n",
    "  \n",
    "  Parámetros:\n",
    "  - h: estado oculto (memoria) inicial\n",
    "  - seed_ix: índice del carácter semilla para comenzar\n",
    "  - n: cantidad de caracteres a generar\n",
    "  \n",
    "  Retorna:\n",
    "  - lista de índices de caracteres generados\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1 # codificar caracter inicial en one-hot\n",
    "  ixes = []\n",
    "  \n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh) # actualizar estado oculto\n",
    "    y = np.dot(Why, h) + by # calcular salida\n",
    "    p = np.exp(y) / np.sum(np.exp(y)) # convertir a probabilidades con softmax\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel()) # muestrear siguiente caracter\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1 # actualizar entrada para el siguiente paso\n",
    "    ixes.append(ix)\n",
    "  \n",
    "  return ixes\n",
    "\n",
    "# Variables para el entrenamiento\n",
    "n, p = 0, 0 # n: contador de iteraciones, p: puntero de posición en los datos\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # variables de memoria para Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # perdida inicial estimada\n",
    "\n",
    "# Bucle de entrenamiento infinito\n",
    "while True:\n",
    "  # Preparar entradas (recorremos el texto de izquierda a derecha en pasos de seq_length)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reiniciar memoria de la RNN\n",
    "    p = 0 # volver al inicio de los datos\n",
    "  \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # secuencia de entrada\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # secuencia objetivo (siguiente caracter)\n",
    "\n",
    "  # Generar texto de muestra cada 100 iteraciones\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200) # generar 200 caracteres\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # Calcular perdida y gradientes\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001 # suavizar la perdida para visualizacion\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # mostrar progreso\n",
    "  \n",
    "  # Actualizar parametros usando Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam # acumular cuadrados de gradientes\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # actualizacion Adagrad\n",
    "\n",
    "  p += seq_length # mover puntero de datos\n",
    "  n += 1 # incrementar contador de iteraciones \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146451b-34f7-482e-80d6-b27d103be243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
