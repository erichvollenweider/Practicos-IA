{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a71651-027f-4016-94a0-f101e4ec5b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 46 characters, 20 unique.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Entrada/Salida de datos\n",
    "data = open('input.txt', 'r').read() # archivo de texto plano de entrada\n",
    "chars = list(set(data)) # caracteres unicos en el archivo de entrada\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) } # diccionario: caracter -> indice\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) } # diccionario: indice -> caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f6f692-9c38-4b77-9603-918ff5dd3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros\n",
    "hidden_size = 100 # tamaño de la capa oculta (cantidad de neuronas)\n",
    "seq_length = 25 # numero de pasos para desenrollar la RNN (longitud de secuencia)\n",
    "learning_rate = 1.e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de224219-d880-4a0e-8017-e439e4480fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN COMUN PELADA COMO LA DEL ERICH\n",
    "# parametros del modelo (matrices de pesos y sesgos)\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # pesos: entrada -> capa oculta\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # pesos: capa oculta -> capa oculta (memoria)\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # pesos: capa oculta -> salida\n",
    "bh = np.zeros((hidden_size, 1)) # sesgo de la capa oculta\n",
    "by = np.zeros((vocab_size, 1)) # sesgo de la capa de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb0ba3c4-5c3d-4a2a-a858-e09e3bd3d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  Calcula la pérdida (loss) y los gradientes para actualizar los pesos.\n",
    "  \n",
    "  Parámetros:\n",
    "  - inputs: lista de índices de caracteres de entrada\n",
    "  - targets: lista de índices de caracteres objetivo (siguiente carácter)\n",
    "  - hprev: estado oculto anterior (memoria de la red)\n",
    "  \n",
    "  Retorna:\n",
    "  - loss: valor de la función de pérdida\n",
    "  - gradientes para cada parámetro (dWxh, dWhh, dWhy, dbh, dby)\n",
    "  - último estado oculto\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {} # diccionarios para almacenar valores en cada paso temporal\n",
    "  hs[-1] = np.copy(hprev) # inicializar con el estado oculto anterior\n",
    "  loss = 0\n",
    "  \n",
    "  # Propagacion hacia adelante (forward pass)\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # codificar entrada en representacion one-hot\n",
    "    xs[t][inputs[t]] = 1 # poner un 1 en la posicion del caracter actual\n",
    "\n",
    "    # aca se rompe, con rnn comun, lo que hay que hacer aca es update gate y reset gate\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # calcular estado oculto\n",
    "      \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # calcular salida (logits sin normalizar)\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # aplicar softmax: convertir a probabilidades\n",
    "    loss += -np.log(ps[t][targets[t],0]) # calcular perdida con entropia cruzada\n",
    "  \n",
    "  # Propagacion hacia atras (backward pass): calcular gradientes en orden inverso\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0]) # gradiente del siguiente estado oculto\n",
    "  \n",
    "  for t in reversed(range(len(inputs))): # recorrer desde el final hasta el inicio\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # gradiente de la funcion softmax con entropia cruzada\n",
    "    dWhy += np.dot(dy, hs[t].T) # gradiente de Why\n",
    "    dby += dy # gradiente de by\n",
    "    dh = np.dot(Why.T, dy) + dhnext # gradiente que llega a la capa oculta\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # gradiente a traves de la funcion tanh\n",
    "    dbh += dhraw # gradiente de bh\n",
    "    dWxh += np.dot(dhraw, xs[t].T) # gradiente de Wxh\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) # gradiente de Whh\n",
    "    dhnext = np.dot(Whh.T, dhraw) # gradiente para el paso temporal anterior\n",
    "  \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # recortar gradientes para evitar explosion\n",
    "  \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c04ac81-c6a0-4f27-8e22-78ea4e8d95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  Genera una secuencia de caracteres del modelo entrenado.\n",
    "  \n",
    "  Parámetros:\n",
    "  - h: estado oculto (memoria) inicial\n",
    "  - seed_ix: índice del carácter semilla para comenzar\n",
    "  - n: cantidad de caracteres a generar\n",
    "  \n",
    "  Retorna:\n",
    "  - lista de índices de caracteres generados\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1 # codificar caracter inicial en one-hot\n",
    "  ixes = []\n",
    "  \n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh) # actualizar estado oculto\n",
    "    y = np.dot(Why, h) + by # calcular salida\n",
    "    p = np.exp(y) / np.sum(np.exp(y)) # convertir a probabilidades con softmax\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel()) # muestrear siguiente caracter\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1 # actualizar entrada para el siguiente paso\n",
    "    ixes.append(ix)\n",
    "  \n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a662a90-0e43-46c5-99cd-285074b93530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables para el entrenamiento\n",
    "n, p = 0, 0 # n: contador de iteraciones, p: puntero de posición en los datos\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # variables de memoria para Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # perdida inicial estimada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e85a952-cebe-4876-9d8e-80b581615ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " nfixapcufnroaRTiftptiNnbomtrNfmdfxdmNon xmupunNfTptmxnTmxtbunddvmRoNxaTppTpnnrxRcoNvtapboarcvppxxabuaTdTrxcdftrcupfeopRf NcdReadpoexnbuvTrdcuuT trbNxfotvuiiifurnria udirpox uevmpaefvxo xaimvdemRTRpbaN \n",
      "----\n",
      "iter 0, loss: 74.893305\n",
      "----\n",
      " aTouccNNcaNvpNneNucTbNeacdcNueiTTfNcpptT xvNmbf xcvoTNxxixouieitffrpibNfdp rnaprefdxm cedmoopttrRvtcotacmvNeniboarrxeNmrdbTcdfTdptfrunuxvNxximv vtuemecNTR dafnRtcvaaodmutdTRfiTTNxmmcrao eTNxfaTcfuTcRa \n",
      "----\n",
      "iter 100, loss: 74.885706\n",
      "----\n",
      " RxufmebRtrmnixTofvcTiouRivovnobpxueotNrenmNobrnvceii xncbbtntnt b Nofb tvdTcorrmtnntuubrrppuoRpTTu R uxarxNcueuvmupnRatc uxTimNxcecRmeefbTpxncunrd dnaabeemtiTm TpccactciumidmxtiraxvxrraRruemaibNavndev \n",
      "----\n",
      "iter 200, loss: 74.871125\n",
      "----\n",
      " RivvdimdetnntTveeboxeNicxpTRmNiabb  mTemNtxcaoeTfbxRRoRcodNnTTTRtoprcuTtdpnTbffbneppfcaptcfmeupNnRopcoNpmrvep eRNubrd vrduRdT unifip niiaoxNpnRRxvdnvcuamxuxaRexcvottNximRxtrnbNRvx mdtounTmpcTmvrbRrnxx \n",
      "----\n",
      "iter 300, loss: 74.851429\n",
      "----\n",
      " fofbTtcvntuTar  aomo TxaiaT TfatatpcrRnbpcNbiouxc iibui ftmxe aprN vT euRdfmotvaoTbbd dTTdcanTfbnrTuxrxoirNdoxdaRxfcp fTRTnfmcbo tmxcnfdRxpdcdppmtefnmaTtcTnfxxoRxfxireaa xpborTxcvtfrpNNmb tnevxraiRfna \n",
      "----\n",
      "iter 400, loss: 74.826718\n",
      "----\n",
      " NntcNe uvxdnabvnefTxTepuabuoccnmdtuduRnmNTfRexbb TuNiibTtrxtT dRprrctrNTNRarrnNrmR miboamfdNbdTiipbdtcmocmbTtmpfftRnT TTvcc NaxpmioiipvvbvvbucurpoufmvnuvRtNxTeRcepovn aebNRnxrcnN Tpron afabtctniurTobb \n",
      "----\n",
      "iter 500, loss: 74.796157\n",
      "----\n",
      " nNpfvuRpaocRRTbRiu Rcev RTd rm cccTtprxdfac ointv odbTixfveTmTbumoiuiTNTRmdNcpmrToxnofrbexpcRuvNNiddTpviooeoineixobpRubTNeoeaxmiNtmTifNfpiapabnxrcetuofnTNRuuvfrvfdpre oavTbTatbaafvieaRxNNpvNrtrmdrcoxd \n",
      "----\n",
      "iter 600, loss: 74.757567\n",
      "----\n",
      " xiruvxTxufcecriaNffxrvoaueaccbxcpatRnvr diueeNxmxdb uvexeTfioxdcexaTua rodtiuxdTantp viiNvRbNrnaonxupvpmpfNc vnt  RRneNpmupRupvmiriuiacRRdbtTo oaxTRdpavrvcctpt ipmftfmevbxTTberRfitvomiiacuNTdfexecbtno \n",
      "----\n",
      "iter 700, loss: 74.705833\n",
      "----\n",
      " mtNRceiNNi vabrdmd actxpeexa r  nNrppt inRiduRpaNeov NrobedrpcraaNprTvpdtRRncbimRdfouuvfbbrxxdt bnbiumabpNrmbf utrfnbbppNexuTdinaxumdccNTeTurmRiupRux Tnfa itnpddmnrrxtTmaebiRxoxTroTNpxNdfNTdvtcffRoa m \n",
      "----\n",
      "iter 800, loss: 74.627042\n",
      "----\n",
      " NNapecvdvxRdNaootuetadutxeT btTcfapvftamtdrt xntaiexfo bRdvn oiTmfTbrbtivobaemfot amtTpdt iecbTdeTedtcxabxfNRapxTNeRvaaammttTndovpimnxovpTaecbamcaTtebTbctft nxrdpoemnnuxt efbxvdmuirduiRoarvnNpecceiunr \n",
      "----\n",
      "iter 900, loss: 74.467638\n",
      "entrenamiento RNN comun terminado!\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 1000\n",
    "\n",
    "while n < max_iterations:\n",
    "  # Si es el primer entrenamiento o si se pasa ya la secuencia se reinicia la capa oculta los pesos\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reiniciar memoria de la RNN\n",
    "    p = 0 # volver al inicio de los datos\n",
    "\n",
    "  # inputs son los caracteres pero como indice(numero), targets es uno adelante de inputs, con tamanio de secuencia de length, es lo que queremos predecir\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # secuencia de entrada\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # secuencia objetivo (siguiente caracter)\n",
    "\n",
    "  # Generar texto de muestra cada 100 iteraciones, con la funcion sample, para ver como empieza a aprender patrones\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200) # generar 200 caracteres\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # Calcular perdida y gradientes\n",
    "  # loss perdida, \n",
    "  # gradientes dWxh,dWhh, dWhy, dbh, dby\n",
    "  # hprev el ultimo estado oculto, que se usa como entrada en la proxima iteracion (memoria)\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    \n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001 # suavizar la perdida para visualizacion\n",
    "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # mostrar progreso\n",
    "  \n",
    "  # Actualizar parametros usando Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam # acumular cuadrados de gradientes\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # actualizacion Adagrad\n",
    "\n",
    "  p += seq_length # mover puntero de datos\n",
    "  n += 1 # incrementar contador de iteraciones\n",
    "\n",
    "print(\"entrenamiento RNN comun terminado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fbbef3-3b4d-4a0a-9e0a-e9d3147184b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU (con puertas de actualizacion, reseteo y candidato oculto)\n",
    "# Donde luego tenemos combinacion final de salida\n",
    "\n",
    "# puerta de actualizacion, es una decision de cuanto del pasado sirve, se mantiene\n",
    "Wxz = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Whz = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "# puerta de reinicio, es una decision de cuanto del pasado se ignora\n",
    "Wxr = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Whr = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "br = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Candidato oculto\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Capa de salida, evitando la explosion/desaparicion del gradiente\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f0e096-8ce1-474a-94ee-01b9d4132a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convierte cualquier valor a rango 0 y 1, usada en las puertas\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c679ae2-22b6-4db3-abd9-6daad22a0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  Calcula la pérdida y los gradientes para entrenar la GRU.\n",
    "  \n",
    "  ¿CÓMO FUNCIONA UNA GRU?\n",
    "  En cada paso temporal t, la GRU decide inteligentemente:\n",
    "  \n",
    "  1. z_t (update gate): \"¿Cuánto mantener del pasado vs. cuánto actualizar?\"\n",
    "     z_t = sigmoid(Wxz*x_t + Whz*h_{t-1} + bz)\n",
    "     Valor cercano a 0 = mantener estado anterior\n",
    "     Valor cercano a 1 = usar nueva información\n",
    "  \n",
    "  2. r_t (reset gate): \"¿Cuánto del pasado es relevante para el nuevo candidato?\"\n",
    "     r_t = sigmoid(Wxr*x_t + Whr*h_{t-1} + br)\n",
    "     Valor cercano a 0 = ignorar estado anterior\n",
    "     Valor cercano a 1 = usar completamente estado anterior\n",
    "  \n",
    "  3. h_tilde (candidato): \"¿Cuál sería el nuevo estado si actualizamos?\"\n",
    "     h_tilde = tanh(Wxh*x_t + Whh*(r_t ⊙ h_{t-1}) + bh)\n",
    "     Nota: r_t filtra selectivamente la información del pasado\n",
    "  \n",
    "  4. h_t (nuevo estado): \"Interpolación entre pasado y candidato\"\n",
    "     h_t = (1 - z_t) ⊙ h_{t-1} + z_t ⊙ h_tilde\n",
    "     z_t controla la mezcla: 0=todo pasado, 1=todo nuevo\n",
    "  \n",
    "  Parámetros:\n",
    "  - inputs: lista de índices de caracteres de entrada [seq_length]\n",
    "  - targets: lista de índices de caracteres objetivo [seq_length] \n",
    "  - hprev: estado oculto anterior (memoria inicial) [hidden_size, 1]\n",
    "  \n",
    "  Retorna:\n",
    "  - loss: valor de pérdida (cross-entropy)\n",
    "  - gradientes: dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby\n",
    "  - hprev: último estado oculto (para siguiente iteración)\n",
    "  \"\"\"\n",
    "  # Diccionarios para guardar valores en cada paso temporal\n",
    "  xs, hs, hs_tilde, zs, rs, ys, ps = {}, {}, {}, {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)  # Estado inicial\n",
    "  loss = 0\n",
    "  \n",
    "  # ============================================================================\n",
    "  # FORWARD PASS: Procesar la secuencia de izquierda a derecha\n",
    "  # ============================================================================\n",
    "  for t in range(len(inputs)):\n",
    "    # --- Codificar entrada como vector one-hot ---\n",
    "    # Ejemplo: si 'e' es índice 4 y vocab_size=21 → [0,0,0,0,1,0,0,...]\n",
    "    xs[t] = np.zeros((vocab_size, 1))\n",
    "    xs[t][inputs[t]] = 1\n",
    "    \n",
    "    # --- 1. Compuerta de actualización: ¿Cuánto actualizar? ---\n",
    "    # Valores entre 0 (mantener todo el pasado) y 1 (usar toda la info nueva)\n",
    "    zs[t] = sigmoid(np.dot(Wxz, xs[t]) + np.dot(Whz, hs[t-1]) + bz)\n",
    "    \n",
    "    # --- 2. Compuerta de reset: ¿Cuánto del pasado usar? ---\n",
    "    # Valores entre 0 (olvidar pasado) y 1 (recordar todo el pasado)\n",
    "    rs[t] = sigmoid(np.dot(Wxr, xs[t]) + np.dot(Whr, hs[t-1]) + br)\n",
    "    \n",
    "    # --- 3. Candidato de estado: ¿Qué información nueva proponer? ---\n",
    "    # rs[t] * hs[t-1] filtra selectivamente el pasado antes de usarlo\n",
    "    # tanh mantiene valores en rango [-1, 1] para estabilidad\n",
    "    hs_tilde[t] = np.tanh(np.dot(Wxh, xs[t]) + \n",
    "                          np.dot(Whh, rs[t] * hs[t-1]) + bh)\n",
    "    \n",
    "    # --- 4. Nuevo estado: Mezcla inteligente de pasado y candidato ---\n",
    "    # Si z=0.3 → 70% del estado anterior + 30% del candidato nuevo\n",
    "    # Esto permite que la red aprenda a mantener información relevante por mucho tiempo\n",
    "    hs[t] = (1 - zs[t]) * hs[t-1] + zs[t] * hs_tilde[t]\n",
    "    \n",
    "    # --- 5. Calcular predicción de siguiente carácter ---\n",
    "    ys[t] = np.dot(Why, hs[t]) + by  # Puntuaciones sin normalizar (logits)\n",
    "    \n",
    "    # Convertir logits a probabilidades con softmax\n",
    "    # Ejemplo: [2.1, 0.5, 3.2] → [0.28, 0.05, 0.67] (suman 1.0)\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "    \n",
    "    # --- 6. Calcular pérdida (cross-entropy) ---\n",
    "    # Si predijo correctamente (p≈1) → loss≈0 (bueno)\n",
    "    # Si predijo mal (p≈0) → loss→∞ (malo)\n",
    "    # El -log penaliza más las predicciones muy confiadas pero incorrectas\n",
    "    loss += -np.log(ps[t][targets[t], 0])\n",
    "  \n",
    "  # ============================================================================\n",
    "  # BACKWARD PASS: Calcular gradientes para ajustar los pesos\n",
    "  # ============================================================================\n",
    "  # Inicializar todos los gradientes en cero\n",
    "  dWxz, dWhz, dbz = np.zeros_like(Wxz), np.zeros_like(Whz), np.zeros_like(bz)\n",
    "  dWxr, dWhr, dbr = np.zeros_like(Wxr), np.zeros_like(Whr), np.zeros_like(br)\n",
    "  dWxh, dWhh, dbh = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(bh)\n",
    "  dWhy, dby = np.zeros_like(Why), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])  # Gradiente que fluye desde el futuro\n",
    "  \n",
    "  # Procesar en orden INVERSO porque los gradientes fluyen hacia atrás en el tiempo\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    \n",
    "    # --- Gradiente de la capa de salida ---\n",
    "    # dy es la derivada de la función de pérdida respecto a las predicciones\n",
    "    # Para softmax + cross-entropy: dy = p - y_verdadero\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1  # Restar 1 solo en la posición del carácter correcto\n",
    "    \n",
    "    # Acumular gradientes para Why y by\n",
    "    dWhy += np.dot(dy, hs[t].T)  # ¿Cómo cambiar Why para reducir error?\n",
    "    dby += dy                     # ¿Cómo cambiar by para reducir error?\n",
    "    \n",
    "    # --- Gradiente que llega al estado oculto ---\n",
    "    # Tiene dos fuentes: 1) desde la salida actual, 2) desde el futuro (dhnext)\n",
    "    dh = np.dot(Why.T, dy) + dhnext\n",
    "    \n",
    "    # --- Gradiente del candidato h_tilde ---\n",
    "    # El candidato contribuyó al estado final proporcionalmente a z\n",
    "    dh_tilde = dh * zs[t]\n",
    "    # Derivada de tanh: d/dx[tanh(x)] = 1 - tanh²(x)\n",
    "    dh_tilde_raw = (1 - hs_tilde[t] * hs_tilde[t]) * dh_tilde\n",
    "    \n",
    "    # Acumular gradientes del candidato\n",
    "    dbh += dh_tilde_raw\n",
    "    dWxh += np.dot(dh_tilde_raw, xs[t].T)\n",
    "    dWhh += np.dot(dh_tilde_raw, (rs[t] * hs[t-1]).T)\n",
    "    \n",
    "    # --- Gradiente de la compuerta de reset ---\n",
    "    # La reset gate afectó cuánto del pasado usar en el candidato\n",
    "    dr = np.dot(Whh.T, dh_tilde_raw) * hs[t-1]\n",
    "    # Derivada de sigmoid: d/dx[σ(x)] = σ(x) * (1 - σ(x))\n",
    "    dr_raw = dr * rs[t] * (1 - rs[t])\n",
    "    \n",
    "    # Acumular gradientes de reset gate\n",
    "    dbr += dr_raw\n",
    "    dWxr += np.dot(dr_raw, xs[t].T)\n",
    "    dWhr += np.dot(dr_raw, hs[t-1].T)\n",
    "    \n",
    "    # --- Gradiente de la compuerta de actualización ---\n",
    "    # La update gate decidió cuánto usar del candidato vs. del pasado\n",
    "    # Si el candidato era mejor que el pasado, z debería aumentar\n",
    "    dz = dh * (hs_tilde[t] - hs[t-1])\n",
    "    dz_raw = dz * zs[t] * (1 - zs[t])  # Derivada de sigmoid\n",
    "    \n",
    "    # Acumular gradientes de update gate\n",
    "    dbz += dz_raw\n",
    "    dWxz += np.dot(dz_raw, xs[t].T)\n",
    "    dWhz += np.dot(dz_raw, hs[t-1].T)\n",
    "    \n",
    "    # --- Calcular gradiente para el paso temporal anterior ---\n",
    "    # Este gradiente tiene 4 fuentes:\n",
    "    # 1) Desde la mezcla directa (1-z)\n",
    "    dhnext = dh * (1 - zs[t])\n",
    "    # 2) Desde la update gate\n",
    "    dhnext += np.dot(Whz.T, dz_raw)\n",
    "    # 3) Desde la reset gate\n",
    "    dhnext += np.dot(Whr.T, dr_raw)\n",
    "    # 4) Desde el candidato (filtrado por reset)\n",
    "    dhnext += np.dot(Whh.T, dh_tilde_raw) * rs[t]\n",
    "  \n",
    "  # --- Gradient clipping: Prevenir explosión de gradientes ---\n",
    "  # Si un gradiente es muy grande (>5) o muy negativo (<-5), recortarlo\n",
    "  # Esto evita actualizaciones descontroladas que arruinen el entrenamiento\n",
    "  for dparam in [dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam)\n",
    "  \n",
    "  return loss, dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509c5017-c52c-4edd-b8be-b6a4f29d5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"\n",
    "  Genera texto usando el modelo entrenado.\n",
    "  \n",
    "  ¿CÓMO GENERA TEXTO LA GRU?\n",
    "  1. Comienza con un carácter semilla (seed_ix)\n",
    "  2. En cada paso:\n",
    "     - Procesa el carácter actual con las compuertas GRU\n",
    "     - Actualiza su estado interno (memoria)\n",
    "     - Predice probabilidades para el siguiente carácter\n",
    "     - Selecciona aleatoriamente según esas probabilidades\n",
    "     - Usa ese carácter como entrada del siguiente paso\n",
    "  3. Repite n veces\n",
    "  \n",
    "  La generación es ESTOCÁSTICA (no determinística) porque:\n",
    "  - np.random.choice muestrea según probabilidades\n",
    "  - Esto crea variedad: misma semilla → textos diferentes\n",
    "  - Palabras/frases comunes se generan más frecuentemente\n",
    "  \n",
    "  Parámetros:\n",
    "  - h: estado oculto inicial [hidden_size, 1]\n",
    "  - seed_ix: índice del carácter para empezar\n",
    "  - n: cantidad de caracteres a generar\n",
    "  \n",
    "  Retorna:\n",
    "  - lista de índices de caracteres generados\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1  # Codificar carácter semilla en one-hot\n",
    "  ixes = []\n",
    "  \n",
    "  for t in range(n):\n",
    "    # Aplicar las tres operaciones de la GRU (igual que en entrenamiento)\n",
    "    z = sigmoid(np.dot(Wxz, x) + np.dot(Whz, h) + bz)        # Update gate\n",
    "    r = sigmoid(np.dot(Wxr, x) + np.dot(Whr, h) + br)        # Reset gate\n",
    "    h_tilde = np.tanh(np.dot(Wxh, x) + np.dot(Whh, r * h) + bh)  # Candidato\n",
    "    h = (1 - z) * h + z * h_tilde                            # Nuevo estado\n",
    "    \n",
    "    # Generar predicción\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))  # Probabilidades para cada carácter\n",
    "    \n",
    "    # Muestrear siguiente carácter según las probabilidades\n",
    "    # Si p=['a':0.7, 'b':0.2, 'c':0.1] → 'a' se elige 70% del tiempo\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    \n",
    "    # Preparar entrada para el siguiente paso\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  \n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2316dbec-0391-4b57-9112-2937a558d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables de entrenamiento inicializadas\n",
      "Pérdida inicial esperada: 74.89\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0  # n = contador de iteraciones, p = posición actual en el texto\n",
    "\n",
    "# Variables de memoria para ADAGRAD (optimizador adaptativo)\n",
    "# Adagrad ajusta el learning rate individualmente para cada parámetro:\n",
    "# - Parámetros con gradientes grandes → learning rate más pequeño\n",
    "# - Parámetros con gradientes pequeños → learning rate más grande\n",
    "# Esto ayuda a convergir más rápido y de forma más estable\n",
    "mWxz, mWhz, mbz = np.zeros_like(Wxz), np.zeros_like(Whz), np.zeros_like(bz)\n",
    "mWxr, mWhr, mbr = np.zeros_like(Wxr), np.zeros_like(Whr), np.zeros_like(br)\n",
    "mWxh, mWhh, mbh = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(bh)\n",
    "mWhy, mby = np.zeros_like(Why), np.zeros_like(by)\n",
    "\n",
    "# Pérdida suavizada para visualización (evita fluctuaciones bruscas)\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length  # Pérdida esperada si predijera al azar\n",
    "\n",
    "print(\"Variables de entrenamiento inicializadas\")\n",
    "print(f\"Pérdida inicial esperada: {smooth_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26196f3d-b16d-43fc-a75a-1b76d64b3278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " ofpoRxutipNRurcoaiRcbndtntacrNox cpvpxvrbNniocbTvtrboNiu NmnaaT t dTmremncbcTtnpraRTdnoinrt c oRxrroiRorcfiRb  bdcvuuervx fdrivirpmoevin iomNurveoxi rxmnRiin eoc acvptu fmna axitepeRinene RbncmartoRpN \n",
      "----\n",
      "iter 0, loss: 74.893182\n",
      "----\n",
      " RcNvNTrcoiuxmap pdpefiofrrveiR mibNcrdvvNuupvnNa ornrvdeaxedTepm aRtmntpmafeermxrdtf RurRTeu rveamcri RuxpmrcoT emvRpudtccNxbibudmodio dTtramedm pxfNrerdn pNnaRvccprnfTippnraxdpnnNinbcRtoRv bxRviuaoct \n",
      "----\n",
      "iter 100, loss: 74.875727\n",
      "----\n",
      " nnvbodmRneNpedcutvNermiRvpdfRdd u xvtxoadxebmRRpTpRpTmc btv mdtTfvnRmupvdbdTnoenaxupubtNRRrfoiiRxamxecrNRbeebRpmrTtvpbnntpxrmaTxofeeiNeiacuubpdeNtfuRriNiNtdcdafpr NbvoxrfaprvcmopRpacuttbtaabtbTc rxaTo \n",
      "----\n",
      "iter 200, loss: 74.854022\n",
      "----\n",
      " arfboRbiRvepdaanppToRffdoeuafRmrtmudefxxupfapbuToNur nnboovNxRn nNvafNiubNRbreircfoTmbmuprupvvntvpRetNNtRmcvtNppdpiNatmivnntcnpnpeaxcavTcux orcpeNfR rvdctRRixroxtcavtrriduToNRnoaaxuepvnutpvr RrpentcbT \n",
      "----\n",
      "iter 300, loss: 74.829354\n",
      "----\n",
      " puiuamidddepcvrNmRtToNitnm mRvvbRuoec o dbbTRmpptx ptaNfurnemNvamaRNRauvrfNxrnddTmcuvbpTbenNRoavRfm fmbeaatrRpdfta c omt reipuapud eTNdTbxxccormeoTTRRvNid vxcbvTtiTexmmNocTarfupTTuvcRacRmexcacuadmxNpR \n",
      "----\n",
      "iter 400, loss: 74.801933\n",
      "----\n",
      " biafnmfNmbamatunvxmx TciTfpNNRdtnrupNcnarripbxRvaeefifcprarvirt frpNeetd ppxtRpmrdrdfbNiupdo ceu avt pa pdomcToNRNbnevueninarmpvciobxbvtnaNfRreaRu xcvx cbbcnpvbR vxxRvoobacbvRxNvmipvceNivpficaidueNnpb \n",
      "----\n",
      "iter 500, loss: 74.771675\n",
      "----\n",
      " caoxcreorbnobuoRpnmRimcRbRaTTt fbmpteRiRencvddxvxdabRrdriiNTafTipmntdxuunvdnddbdtTnuocexivnaaaeippviaTmiTrNcectoioccibbmbv  fxtcbftxxRrboprpuvuxndtbNbmRtNdoteTnornxeNabvoftRmxva tpaRnN RnfnTexnuioNiRo \n",
      "----\n",
      "iter 600, loss: 74.738346\n",
      "----\n",
      " ctvTocvaboeimdovbRtcnoRmdTnnxctabtue itcarxditTxticTdmtdniapammTcTRm uNfNadRuoNuttfpRibiadaniTvvNffoNcT exmedcexu TaRtcuprfitotbmbvNcotNrfRaNRpitdTnxuRmmenNutadcTpafopo nondmrdrbRvatativiTanmc undnvoo \n",
      "----\n",
      "iter 700, loss: 74.701609\n",
      "----\n",
      " RnimTdepNxbndnbbRcdmobcTTuieraNpapbxf xNxbiRnvrepodRxrrorvmRxteReidiadive exNatNtn iviupvurpuitpa oibtReiNpxNNfxprvtfNrtexiopcaoTRmft rbcmfemnminNpnRfNiiufRcrNbRpvbcfcNTmdmm rfeootioTfdpcvTTv uatbu mp \n",
      "----\n",
      "iter 800, loss: 74.661032\n",
      "----\n",
      " b cdxixvaptxNprTviTrdNtRReiRxxRbTieptNevpovvRfeNaovvrdrfcmNbTfcodffocarapnpT Tin tNfNvnxdvmNbaecformxdRT uRxaNibotRNbudimcvcduoiuobuxrxmRp ppTotvTuutNtv dmr d NcievedRtvmdabattfideRubiftmoRRomcinaouTR \n",
      "----\n",
      "iter 900, loss: 74.616081\n",
      "Entrenamiento GRU completado!\n"
     ]
    }
   ],
   "source": [
    "max_iterations = 1000\n",
    "\n",
    "while n < max_iterations:\n",
    "  # --- Reiniciar si llegamos al final del texto o es la primera iteración ---\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1))  # Resetear memoria de la GRU\n",
    "    p = 0  # Volver al inicio del texto\n",
    "  \n",
    "  # --- Preparar secuencias de entrada y objetivo ---\n",
    "  # Entrada: caracteres en posiciones [p, p+1, ..., p+seq_length-1]\n",
    "  # Objetivo: caracteres en posiciones [p+1, p+2, ..., p+seq_length]\n",
    "  # Ejemplo: si seq_length=3 y texto=\"hola\"\n",
    "  #   inputs = ['h', 'o', 'l'] → targets = ['o', 'l', 'a']\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # --- Generar y mostrar texto de muestra cada 100 iteraciones ---\n",
    "  # Esto nos permite ver cómo mejora el modelo durante el entrenamiento\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)  # Generar 200 caracteres\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # --- Calcular pérdida y gradientes ---\n",
    "  loss, dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby, hprev = \\\n",
    "      lossFun(inputs, targets, hprev)\n",
    "  \n",
    "  # Suavizar pérdida: 99.9% del valor anterior + 0.1% del valor actual\n",
    "  # Esto crea una curva suave para ver tendencias sin ruido\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: \n",
    "    print('iter %d, loss: %f' % (n, smooth_loss))\n",
    "  \n",
    "  # --- Actualizar parámetros usando Adagrad ---\n",
    "  # Fórmula: θ = θ - (learning_rate / √(sum_gradientes² + ε)) * gradiente\n",
    "  # El denominador aumenta con el tiempo → pasos más pequeños → convergencia estable\n",
    "  params = [Wxz, Whz, bz, Wxr, Whr, br, Wxh, Whh, bh, Why, by]\n",
    "  dparams = [dWxz, dWhz, dbz, dWxr, dWhr, dbr, dWxh, dWhh, dbh, dWhy, dby]\n",
    "  mems = [mWxz, mWhz, mbz, mWxr, mWhr, mbr, mWxh, mWhh, mbh, mWhy, mby]\n",
    "  \n",
    "  for param, dparam, mem in zip(params, dparams, mems):\n",
    "    mem += dparam * dparam  # Acumular cuadrado de gradientes\n",
    "    # Actualizar parámetro con learning rate adaptativo\n",
    "    # 1e-8 previene división por cero\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "  p += seq_length  # Avanzar al siguiente bloque de texto\n",
    "  n += 1  # Incrementar contador de iteraciones\n",
    "\n",
    "print(\"Entrenamiento GRU completado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
