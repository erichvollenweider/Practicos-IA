{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fb4b1d-4da8-4615-aa6a-ddcc5dc41432",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Consigna: Práctica 6 Ejercicio 3\n",
    "\n",
    "### Implementación de una CNN para el dataset Iris\n",
    "\n",
    "Implemente una **Red Neuronal Convolucional (CNN)** para el modelo del **dataset Iris**.  \n",
    "Pruebe con diferentes configuraciones: variaciones en la cantidad y tamaño de **capas convolucionales**, así como en la estructura de las **capas densas**.  \n",
    "Registre y compare las métricas. Contraste los resultados con los modelos previos entrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f79a1-1a17-4ced-be01-a366fec42054",
   "metadata": {},
   "source": [
    "# Resolución\n",
    "\n",
    "En este ejercicio se implemento una red neuronal convolucional (CNN) utilizando el clásico y conocido dataset Iris, adaptado a un formato de imágenes, lo que es ideal para aplicar redes convolucionales.\n",
    "El conjunto de datos contiene mediciones de flores pertenecientes a tres especies de Iris (setosa, versicolor y virginica), con el objetivo de predecir correctamente la especie a partir de dichas características.\n",
    "Para la implemetacion se uso el Framework Keras en cambio de sklearn usado en los anteriores ejercicios, se utiliza este porque es mas apropiado para redes convolucionales.\n",
    "Para evaluar el desempeño se realizó una división del dataset en 80% para entrenamiento y 20% para prueba, y se calcularon métricas como accuracy y loss que es proporcionado por el Framework utilizado y se van calculando en cada epoca de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810312c2-d30a-480c-a586-48a9dacbafd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 19:41:19.674044: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-04 19:41:19.674407: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-04 19:41:19.733107: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-04 19:41:21.155137: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-04 19:41:21.155538: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f23113-3116-4d67-acb4-c1d2b3b5a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421 files belonging to 3 classes.\n",
      "Using 337 files for training.\n",
      "Found 421 files belonging to 3 classes.\n",
      "Using 84 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 19:41:22.398269: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Se toma el dataset Iris en imagenes\n",
    "data_dir = \"Datasets/Iris\"\n",
    "\n",
    "# Datos de entrenamiento, 80% de las imagenes\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),  # tamaño al que se redimensionan las imágenes\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Datos de validacion, 20% de las imagenes\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92692a4-ed98-4cda-b9ec-66bf7cc41cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocesamiento de datos, primero se normalizan de 0-255(xq son imagenes a color)\n",
    "# a un normalizado de 0-1.\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_ds = train_data.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_data.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb131ae5-51cf-47e3-a6ff-338ba27f2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distintos filtros como para mejorar el overfitting\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),   # voltea horizontal y verticalmente\n",
    "    layers.RandomRotation(0.2),                     # rotación ±20%\n",
    "    layers.RandomTranslation(0.1, 0.1),             # traslación hasta 10% ancho/alto\n",
    "    layers.RandomZoom(0.15),                        # zoom ±15%\n",
    "    layers.RandomBrightness(0.2),                   # brillo ±20%\n",
    "    layers.RandomContrast(0.2),                     # contraste ±20%\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4436a38d-453f-4c4d-a213-10fd6827224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/Escritorio/uni/2025/2do-cuatri/Inteligencia Artificial/practicos-ia/Practicos-IA/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Creacion del modelo convolucional\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    # Capa convolucional con 32 kernels con cada kernel de 3x3\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)), # agarro las imagenes de 128x128 y 3 canales(rgb)\n",
    "    MaxPooling2D(2,2),\n",
    "    # Capa convolucional con 64 kernels, se va aprendiendo patrones mas complejos, con cada kernel de 3x3\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    # Flatten convierte la salida de 2D en 1D(vector), necesario para capas densas que agarran un vector como entrada\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax') # 3 clases objetivo\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d109ee87-6e8d-456d-9c37-21594b8f659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilacion del modelo con una funcion de activacion, una de perdida y las metricas a utilizar\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # se usa esta porque las etiquetas son enteros (0,1,2)\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4100370c-f15c-4deb-aa75-af6fa4ad7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping para ver si no mejora, cortar el entrenamiento\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# Ver\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3bb7925-7452-4aae-b5cd-dc69e6b96b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.5104 - loss: 24.8518 - val_accuracy: 0.6548 - val_loss: 2.2637 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6142 - loss: 1.1116 - val_accuracy: 0.6548 - val_loss: 0.9345 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6380 - loss: 1.0122 - val_accuracy: 0.6548 - val_loss: 0.9553 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6320 - loss: 1.0015 - val_accuracy: 0.6548 - val_loss: 0.9553 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6350 - loss: 0.9909 - val_accuracy: 0.6548 - val_loss: 0.9421 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.6350 - loss: 0.9651 - val_accuracy: 0.6548 - val_loss: 0.9660 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6320 - loss: 0.9716 - val_accuracy: 0.6548 - val_loss: 0.9576 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.9448 - val_accuracy: 0.6548 - val_loss: 0.9447 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6350 - loss: 0.9464 - val_accuracy: 0.6548 - val_loss: 0.9343 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6350 - loss: 0.9389 - val_accuracy: 0.6548 - val_loss: 0.9291 - learning_rate: 5.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6350 - loss: 0.9263 - val_accuracy: 0.6548 - val_loss: 0.9148 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.6350 - loss: 0.9182 - val_accuracy: 0.6548 - val_loss: 0.8982 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.6350 - loss: 0.9362 - val_accuracy: 0.6548 - val_loss: 0.9078 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6350 - loss: 0.9105 - val_accuracy: 0.6548 - val_loss: 0.9015 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.6350 - loss: 0.9273 - val_accuracy: 0.6548 - val_loss: 0.9022 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6350 - loss: 0.9250 - val_accuracy: 0.6548 - val_loss: 0.9002 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.6350 - loss: 0.9126 - val_accuracy: 0.6548 - val_loss: 0.9000 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.9150 - val_accuracy: 0.6548 - val_loss: 0.8991 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.9126 - val_accuracy: 0.6548 - val_loss: 0.8982 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.9052 - val_accuracy: 0.6548 - val_loss: 0.8977 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.9013 - val_accuracy: 0.6548 - val_loss: 0.8961 - learning_rate: 2.5000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.9153 - val_accuracy: 0.6548 - val_loss: 0.8930 - learning_rate: 2.5000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.8954 - val_accuracy: 0.6548 - val_loss: 0.8757 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.8920 - val_accuracy: 0.6548 - val_loss: 0.8878 - learning_rate: 2.5000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6350 - loss: 0.9197 - val_accuracy: 0.6548 - val_loss: 0.8945 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.6380 - loss: 0.9019 - val_accuracy: 0.6548 - val_loss: 0.8850 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6350 - loss: 0.8849 - val_accuracy: 0.6548 - val_loss: 0.8766 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.6350 - loss: 0.8894 - val_accuracy: 0.6548 - val_loss: 0.8851 - learning_rate: 1.2500e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.6350 - loss: 0.8998 - val_accuracy: 0.6548 - val_loss: 0.8704 - learning_rate: 1.2500e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6350 - loss: 0.9068 - val_accuracy: 0.6548 - val_loss: 0.8873 - learning_rate: 1.2500e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - accuracy: 0.6350 - loss: 0.8980 - val_accuracy: 0.6548 - val_loss: 0.8798 - learning_rate: 1.2500e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6350 - loss: 0.9138 - val_accuracy: 0.6548 - val_loss: 0.8889 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.6350 - loss: 0.8994 - val_accuracy: 0.6548 - val_loss: 0.8888 - learning_rate: 1.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.6350 - loss: 0.8936 - val_accuracy: 0.6548 - val_loss: 0.8879 - learning_rate: 6.2500e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.6380 - loss: 0.8943 - val_accuracy: 0.6548 - val_loss: 0.8872 - learning_rate: 6.2500e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.6350 - loss: 0.8930 - val_accuracy: 0.6548 - val_loss: 0.8871 - learning_rate: 6.2500e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 0.6350 - loss: 0.8979 - val_accuracy: 0.6548 - val_loss: 0.8863 - learning_rate: 6.2500e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.6350 - loss: 0.8959 - val_accuracy: 0.6548 - val_loss: 0.8859 - learning_rate: 3.1250e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.6320 - loss: 0.8945 - val_accuracy: 0.6548 - val_loss: 0.8854 - learning_rate: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=50, # Cantidad de epocas\n",
    "    callbacks=[early_stop, reduce_lr] # Guardar mejor modelo si va empeorando la perdida en validacion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69632729-9587-4e4a-af17-b023642904bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6548 - loss: 0.8704\n",
      "Resultados: [0.8704285621643066, 0.6547619104385376]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate calcula la perdida y la metrica accuracy dadas en compilacion\n",
    "results = model.evaluate(val_data)\n",
    "print(\"Resultados:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309328c8-a3f1-40ad-ac21-8e634254275d",
   "metadata": {},
   "source": [
    "#  Conclusión\n",
    "\n",
    "El modelo alcanzó un accuracy en validación de aproximadamente 0.65, lo que indica que, aunque logra identificar correctamente algunos patrones, presenta un sesgo claro hacia la clase con más ejemplos. Este resultado refleja la importancia de mantener una distribución equilibrada de clases, ya que un dataset desbalanceado tiende a degradar el rendimiento global y reduce la capacidad del modelo para reconocer categorías minoritarias. En definitiva, la calidad de los datos son tan importantes como la arquitectura del modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
