{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582f79a1-1a17-4ced-be01-a366fec42054",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En este ejercicio se implemento una red neuronal convolucional (CNN) utilizando el clásico y conocido dataset Iris, adaptado a un formato de imágenes, lo que es ideal para aplicar redes convolucionales.\n",
    "El conjunto de datos contiene mediciones de flores pertenecientes a tres especies de Iris (setosa, versicolor y virginica), con el objetivo de predecir correctamente la especie a partir de dichas características.\n",
    "Para la implemetacion se uso el Framework Keras en cambio de sklearn usado en los anteriores ejercicios, se utiliza este porque es mas apropiado para redes convolucionales.\n",
    "Para evaluar el desempeño se realizó una división del dataset en 80% para entrenamiento y 20% para prueba, y se calcularon métricas como accuracy y loss que es proporcionado por el Framework utilizado y se van calculando en cada epoca de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810312c2-d30a-480c-a586-48a9dacbafd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:39:16.192426: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-02 19:39:16.308117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-02 19:39:19.292393: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f23113-3116-4d67-acb4-c1d2b3b5a152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 421 files belonging to 3 classes.\n",
      "Using 337 files for training.\n",
      "Found 421 files belonging to 3 classes.\n",
      "Using 84 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 19:39:22.109476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "# Se toma el dataset Iris en imagenes\n",
    "data_dir = \"Datasets/Iris\"\n",
    "\n",
    "# Datos de entrenamiento, 80% de las imagenes\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),  # tamaño al que se redimensionan las imágenes\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Datos de validacion, 20% de las imagenes\n",
    "val_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c92692a4-ed98-4cda-b9ec-66bf7cc41cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocesamiento de datos, primero se normalizan de 0-255(xq son imagenes a color)\n",
    "# a un normalizado de 0-1.\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_ds = train_data.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_data.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb131ae5-51cf-47e3-a6ff-338ba27f2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distintos filtros como para mejorar el overfitting\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),   # voltea horizontal y verticalmente\n",
    "    layers.RandomRotation(0.2),                     # rotación ±20%\n",
    "    layers.RandomTranslation(0.1, 0.1),             # traslación hasta 10% ancho/alto\n",
    "    layers.RandomZoom(0.15),                        # zoom ±15%\n",
    "    layers.RandomBrightness(0.2),                   # brillo ±20%\n",
    "    layers.RandomContrast(0.2),                     # contraste ±20%\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4436a38d-453f-4c4d-a213-10fd6827224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creacion del modelo convolucional\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    # Capa convolucional con 32 kernels con cada kernel de 3x3\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(128,128,3)), # agarro las imagenes de 128x128 y 3 canales(rgb)\n",
    "    MaxPooling2D(2,2),\n",
    "    # Capa convolucional con 64 kernels, se va aprendiendo patrones mas complejos, con cada kernel de 3x3\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    # Flatten convierte la salida de 2D en 1D(vector), necesario para capas densas que agarran un vector como entrada\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax') # 3 clases objetivo\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d109ee87-6e8d-456d-9c37-21594b8f659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilacion del modelo con una funcion de activacion, una de perdida y las metricas a utilizar\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # se usa esta porque las etiquetas son enteros (0,1,2)\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4100370c-f15c-4deb-aa75-af6fa4ad7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping para ver si no mejora, cortar el entrenamiento\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# Ver\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3bb7925-7452-4aae-b5cd-dc69e6b96b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 829ms/step - accuracy: 0.4273 - loss: 5.6999 - val_accuracy: 0.2619 - val_loss: 1.4927 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 711ms/step - accuracy: 0.4985 - loss: 4.1929 - val_accuracy: 0.1667 - val_loss: 3.8063 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 711ms/step - accuracy: 0.4926 - loss: 1.5757 - val_accuracy: 0.1667 - val_loss: 4.3213 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 713ms/step - accuracy: 0.5757 - loss: 1.1464 - val_accuracy: 0.4643 - val_loss: 1.9038 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 736ms/step - accuracy: 0.6024 - loss: 1.0720 - val_accuracy: 0.5357 - val_loss: 1.7620 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 737ms/step - accuracy: 0.6142 - loss: 1.0747 - val_accuracy: 0.6071 - val_loss: 1.8401 - learning_rate: 5.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 719ms/step - accuracy: 0.6172 - loss: 0.9987 - val_accuracy: 0.5714 - val_loss: 2.0999 - learning_rate: 5.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 725ms/step - accuracy: 0.6320 - loss: 0.9793 - val_accuracy: 0.5476 - val_loss: 2.3888 - learning_rate: 5.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 743ms/step - accuracy: 0.6350 - loss: 0.9610 - val_accuracy: 0.5714 - val_loss: 2.6626 - learning_rate: 5.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 732ms/step - accuracy: 0.6350 - loss: 0.9871 - val_accuracy: 0.5595 - val_loss: 2.8902 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 743ms/step - accuracy: 0.6350 - loss: 0.9371 - val_accuracy: 0.5476 - val_loss: 3.1203 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=50, # Cantidad de epocas\n",
    "    callbacks=[early_stop, reduce_lr] # Guardar mejor modelo si va empeorando la perdida en validacion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69632729-9587-4e4a-af17-b023642904bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.2619 - loss: 1.4927\n",
      "Resultados: [1.4927189350128174, 0.261904776096344]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate calcula la perdida y la metrica accuracy dadas en compilacion\n",
    "results = model.evaluate(val_data)\n",
    "print(\"Resultados:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309328c8-a3f1-40ad-ac21-8e634254275d",
   "metadata": {},
   "source": [
    "#  Conclusión"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
